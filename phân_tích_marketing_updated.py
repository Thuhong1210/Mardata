# -*- coding: utf-8 -*-
"""Ph√¢n_t√≠ch_marketing_UPDATED.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14eunN1vcloYJmHAluoZzuqaFCIcFxbWE
"""

# ============================================================
# 0. IMPORT TH∆Ø VI·ªÜN V√Ä ƒê·ªåC D·ªÆ LI·ªÜU
# ============================================================

import pandas as pd
import numpy as np

df = pd.read_csv("onlinebuy.csv")  # ƒë·ªïi ƒë∆∞·ªùng d·∫´n n·∫øu ch·∫°y Colab

print("K√≠ch th∆∞·ªõc d·ªØ li·ªáu:", df.shape)
df.head()

# ============================================================
# 1. M√î T·∫¢ D·ªÆ LI·ªÜU (DESCRIPTIVE STATISTICS)
# ============================================================

# Nh√≥m bi·∫øn Likert (1‚Äì5)
likert_vars = [
    'int1','int2','inf1','inf2','inf3','ve1','ve2','ve3','nvse1','nvse2',
    'trust1','trust2','trust3','conv1','conv2','conv3','conv4',
    'enj1','enj2','enj3','sc1','sc2','al1','al2','al3'
]

desc = df[likert_vars].describe().T
display(desc[['mean','std','min','25%','50%','75%','max']])

print("\nPh√¢n b·ªë gi·ªõi t√≠nh:")
print(df[['gender_0','gender_1']].sum())

print("\nPh√¢n b·ªë nh√≥m tu·ªïi:")
print(df[['age_0','age_1','age_2']].sum())

print("\nPh√¢n b·ªë t·∫ßn su·∫•t mua s·∫Øm:")
print(df[['freq_0','freq_1','freq_2','freq_3']].sum())

# ============================================================
# CELL 1: DATA CLEANING
# ============================================================

print("="*80)
print("üßπ B∆Ø·ªöC 1: DATA CLEANING")
print("="*80)

# Danh s√°ch c√°c c·ªôt Likert c·∫ßn ki·ªÉm tra
likert_cols = [
    'int1','int2','inf1','inf2','inf3','ve1','ve2','ve3','nvse1','nvse2',
    'trust1','trust2','trust3','conv1','conv2','conv3','conv4',
    'enj1','enj2','enj3','sc1','sc2','al1','al2','al3'
]

def check_invalid(row):
    """
    Ki·ªÉm tra d·ªØ li·ªáu kh√¥ng h·ª£p l·ªá:
    1. T·∫•t c·∫£ gi√° tr·ªã gi·ªëng nhau
    2. ‚â•10 gi√° tr·ªã li√™n ti·∫øp gi·ªëng nhau
    """
    vals = row[likert_cols].values

    # Ki·ªÉm tra t·∫•t c·∫£ gi·ªëng nhau
    if len(set(vals)) == 1:
        return True

    # Ki·ªÉm tra 10 li√™n ti·∫øp gi·ªëng nhau
    max_c = current = 1
    for i in range(1, len(vals)):
        if vals[i] == vals[i-1]:
            current += 1
            max_c = max(max_c, current)
        else:
            current = 1
    return max_c >= 10

# T√¨m v√† lo·∫°i b·ªè d·ªØ li·ªáu kh√¥ng h·ª£p l·ªá
print(f"\nüîç Ki·ªÉm tra d·ªØ li·ªáu...")
invalid = [i for i, row in df.iterrows() if check_invalid(row)]

print(f"\nüìä K·∫æT QU·∫¢:")
print(f"   T·ªïng m·∫´u ban ƒë·∫ßu: {len(df)}")
print(f"   M·∫´u kh√¥ng h·ª£p l·ªá: {len(invalid)} ({len(invalid)/len(df)*100:.1f}%)")

if len(invalid) > 0:
    print(f"\n   V√≠ d·ª• 5 m·∫´u ƒë·∫ßu ti√™n b·ªã lo·∫°i: {invalid[:5]}")

# Lo·∫°i b·ªè
df = df.drop(invalid).reset_index(drop=True)

print(f"\n‚úÖ SAU KHI L√ÄM S·∫†CH:")
print(f"   C√≤n l·∫°i: {len(df)} m·∫´u ({len(df)/(len(df)+len(invalid))*100:.1f}%)")
print(f"   ƒê√£ lo·∫°i: {len(invalid)} m·∫´u")

# L∆∞u d·ªØ li·ªáu s·∫°ch
df.to_csv('onlinebuy_cleaned.csv', index=False)
print(f"\nüíæ ƒê√£ l∆∞u: onlinebuy_cleaned.csv")

# ============================================================
# 2. FACTOR ANALYSIS
# ============================================================

print("\nüìä B∆Ø·ªöC 2: FACTOR ANALYSIS")

# C√†i th∆∞ vi·ªán
!pip install factor-analyzer -q

from factor_analyzer import FactorAnalyzer
from factor_analyzer.factor_analyzer import calculate_kmo, calculate_bartlett_sphericity

# KMO
kmo_all, kmo_model = calculate_kmo(df[likert_cols])
print(f"\nKMO Score: {kmo_model:.3f}")

# Bartlett's
chi2, pval = calculate_bartlett_sphericity(df[likert_cols])
print(f"Bartlett's p-value: {pval:.4f}")

# Determine optimal factors
fa_test = FactorAnalyzer(n_factors=25, rotation=None)
fa_test.fit(df[likert_cols])
ev, v = fa_test.get_eigenvalues()
n_factors = sum(ev > 1.0)
print(f"\nS·ªë nh√¢n t·ªë t·ªëi ∆∞u: {n_factors}")

# Run FA
fa = FactorAnalyzer(n_factors=n_factors, rotation='varimax')
fa.fit(df[likert_cols])
loadings_df = pd.DataFrame(
    fa.loadings_,
    index=likert_cols,
    columns=[f'Factor{i+1}' for i in range(n_factors)]
)

print("\nFactor Loadings (|loading| ‚â• 0.4):")
for col in loadings_df.columns:
    high = loadings_df[col][abs(loadings_df[col]) >= 0.4].sort_values(ascending=False)
    if len(high) > 0:
        print(f"\n{col}:")
        for var, val in high.items():
            print(f"  {var}: {val:.3f}")

# ============================================================
# T·∫†O C√ÅC BI·∫æN AGGREGATE - CH·∫†Y CELL N√ÄY TR∆Ø·ªöC REGRESSION!
# ============================================================

print("üîß T·∫°o c√°c bi·∫øn aggregate...")

# Platform Characteristics
df['INT'] = df[['int1','int2']].mean(axis=1)
df['INF'] = df[['inf1','inf2','inf3']].mean(axis=1)
df['VE'] = df[['ve1','ve2','ve3']].mean(axis=1)
df['NVSE'] = df[['nvse1','nvse2']].mean(axis=1)

# Psychological Responses
df['TRUST'] = df[['trust1','trust2','trust3']].mean(axis=1)
df['CONV'] = df[['conv1','conv2','conv3','conv4']].mean(axis=1)
df['ENJ'] = df[['enj1','enj2','enj3']].mean(axis=1)
df['SC'] = df[['sc1','sc2']].mean(axis=1)

# Attitudinal Loyalty
df['AL'] = df[['al1','al2','al3']].mean(axis=1)

print("‚úÖ XONG! ƒê√£ t·∫°o 9 bi·∫øn: INT, INF, VE, NVSE, TRUST, CONV, ENJ, SC, AL")
print(f"\nüìä Th·ªëng k√™:")
print(df[['INT', 'INF', 'VE', 'NVSE', 'TRUST', 'CONV', 'ENJ', 'SC', 'AL']].describe().T[['mean','std']].round(3))

# ============================================================
# 3. REGRESSION
# ============================================================

print("\nüìà B∆Ø·ªöC 3: REGRESSION ANALYSIS")

import statsmodels.api as sm

# T·∫°o bi·∫øn
df['AL'] = df[['al1','al2','al3']].mean(axis=1)

# Regression 1
X1 = sm.add_constant(df[['INT','INF','VE','NVSE']])
model1 = sm.OLS(df['AL'], X1).fit()
print(f"\nRegression 1: R¬≤ = {model1.rsquared:.4f}")
print(model1.summary())

# Regression 2
X2 = sm.add_constant(df[['TRUST','CONV','ENJ','SC']])
model2 = sm.OLS(df['AL'], X2).fit()
print(f"\nRegression 2: R¬≤ = {model2.rsquared:.4f}")
print(model2.summary())

print("\n" + "="*80)
print("‚úÖ HO√ÄN TH√ÄNH!")
print("="*80)

# ============================================================
# B·ªî SUNG: T√çNH GI√Å TR·ªä MIN C·ª¶A T·ª™NG THANG ƒêO
# ============================================================
import pandas as pd
import numpy as np

df = pd.read_csv("onlinebuy.csv")

scale_min = df[likert_vars].min()
print(scale_min)

# ============================================================
# 2. CRONBACH'S ALPHA
# ============================================================

def cronbach_alpha(df_scale):
    k = df_scale.shape[1]
    item_var = df_scale.var(axis=0, ddof=1)
    total_var = df_scale.sum(axis=1).var(ddof=1)
    return (k/(k-1))*(1 - item_var.sum()/total_var)

scales = {
    'Interactivity': ['int1','int2'],
    'Informativeness': ['inf1','inf2','inf3'],
    'Visual_Engagement': ['ve1','ve2','ve3'],
    'Navigation_Ease': ['nvse1','nvse2'],
    'Trust': ['trust1','trust2','trust3'],
    'Convenience': ['conv1','conv2','conv3','conv4'],
    'Enjoyment': ['enj1','enj2','enj3'],
    'Self_Control': ['sc1','sc2'],
    'Attitudinal_Loyalty': ['al1','al2','al3']
}

print("\n=== CRONBACH'S ALPHA THEO T·ª™NG THANG ƒêO ===")
for name, cols in scales.items():
    alpha = cronbach_alpha(df[cols])
    print(f"{name}: {alpha:.3f}")

# ============================================================
# 3. EXPLORATORY FACTOR ANALYSIS (EFA)
# ============================================================
!pip install factor_analyzer

from factor_analyzer import FactorAnalyzer
from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo

efa_data = df[likert_vars].dropna()

# Bartlett test & KMO
chi_square, p_value = calculate_bartlett_sphericity(efa_data)
kmo_all, kmo_model = calculate_kmo(efa_data)

print("Bartlett Test p-value:", p_value)
print("KMO:", kmo_model)

# Eigenvalues
fa_ev = FactorAnalyzer(rotation=None)
fa_ev.fit(efa_data)
ev, v = fa_ev.get_eigenvalues()

print("\nEigenvalues:")
print(ev)

# S·ªë nh√¢n t·ªë = s·ªë eigenvalue > 1
n_factors = sum(ev > 1)
print("\nS·ªë nh√¢n t·ªë ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t:", n_factors)

# EFA v·ªõi Varimax
fa = FactorAnalyzer(n_factors=n_factors, rotation="varimax")
fa.fit(efa_data)

loadings = pd.DataFrame(fa.loadings_, index=likert_vars)
print("\nFactor Loadings:")
loadings

# ============================================================
# 4. PEARSON CORRELATION
# ============================================================

# Import th∆∞ vi·ªán
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns   # d√πng heatmap tr·ª±c quan

# T√≠nh ma tr·∫≠n t∆∞∆°ng quan
corr = df[likert_vars].corr()

# Hi·ªÉn th·ªã b·∫£ng t∆∞∆°ng quan
corr

# ============================================================
# 4.1. CORRELATION HEATMAP - Tr·ª±c quan h√≥a ma tr·∫≠n t∆∞∆°ng quan
# ============================================================

# T·∫°o heatmap cho ma tr·∫≠n t∆∞∆°ng quan
plt.figure(figsize=(16, 14))
sns.heatmap(
    corr,
    annot=True,      # Hi·ªÉn th·ªã gi√° tr·ªã s·ªë
    fmt='.2f',       # Format 2 ch·ªØ s·ªë th·∫≠p ph√¢n
    cmap='coolwarm', # M√†u s·∫Øc: ƒë·ªè = t∆∞∆°ng quan d∆∞∆°ng, xanh = t∆∞∆°ng quan √¢m
    center=0,        # Trung t√¢m t·∫°i 0
    square=True,     # √î vu√¥ng
    linewidths=0.5,  # ƒê∆∞·ªùng chia
    cbar_kws={"shrink": 0.8}
)
plt.title('Ma tr·∫≠n t∆∞∆°ng quan Pearson gi·ªØa c√°c bi·∫øn', fontsize=16, pad=20)
plt.tight_layout()
plt.show()

print("\nüìä Gi·∫£i th√≠ch:")
print("- M√†u ƒë·ªè ƒë·∫≠m: T∆∞∆°ng quan d∆∞∆°ng m·∫°nh (g·∫ßn +1)")
print("- M√†u xanh ƒë·∫≠m: T∆∞∆°ng quan √¢m m·∫°nh (g·∫ßn -1)")
print("- M√†u tr·∫Øng: Kh√¥ng c√≥ t∆∞∆°ng quan (g·∫ßn 0)")

# ============================================================
# 5. K-MEANS CLUSTERING
# ============================================================
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# T·∫°o ƒëi·ªÉm trung b√¨nh cho t·ª´ng thang ƒëo
df['INT'] = df[['int1','int2']].mean(axis=1)
df['INF'] = df[['inf1','inf2','inf3']].mean(axis=1)
df['VE'] = df[['ve1','ve2','ve3']].mean(axis=1)
df['NVSE'] = df[['nvse1','nvse2']].mean(axis=1)
df['TRUST'] = df[['trust1','trust2','trust3']].mean(axis=1)
df['CONV'] = df[['conv1','conv2','conv3','conv4']].mean(axis=1)
df['ENJ'] = df[['enj1','enj2','enj3']].mean(axis=1)
df['SC'] = df[['sc1','sc2']].mean(axis=1)

cluster_vars = ['INT','INF','VE','NVSE','TRUST','CONV','ENJ','SC']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(df[cluster_vars])

k = 3  # c√≥ th·ªÉ ƒë·ªïi 2/3/4 c·ª•m
kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
df['cluster'] = kmeans.fit_predict(X_scaled)

print("S·ªë l∆∞·ª£ng trong t·ª´ng c·ª•m:")
print(df['cluster'].value_counts())

print("\nTrung b√¨nh ƒë·∫∑c t√≠nh t·ª´ng c·ª•m:")
df.groupby('cluster')[cluster_vars].mean().round(3)

# ============================================================
# 5.1. ƒê·∫∂T T√äN CHO C√ÅC CLUSTER
# ============================================================

# Xem l·∫°i ƒë·∫∑c ƒëi·ªÉm trung b√¨nh c·ªßa t·ª´ng cluster
cluster_stats = df.groupby('cluster')[cluster_vars].mean().round(3)
print("ƒê·∫∑c ƒëi·ªÉm trung b√¨nh c·ªßa t·ª´ng cluster:")
display(cluster_stats)

# Ph√¢n t√≠ch ƒë·ªÉ ƒë·∫∑t t√™n cluster
print("\n" + "="*70)
print("PH√ÇN T√çCH ƒê·∫∂C ƒêI·ªÇM C·ª§M:")
print("="*70)

# Cluster 0: Purchase Intention, Trust, Convenience, Enjoyment cao
print("\nüéØ Cluster 0: 'Enthusiastic Shoppers' (Ng∆∞·ªùi mua s·∫Øm nhi·ªát t√¨nh)")
print("   - ƒê·∫∑c ƒëi·ªÉm: INT cao (4.594), TRUST cao (3.396), CONV cao (4.152), ENJ cao (4.080)")
print("   - VE trung b√¨nh (2.446), NVSE th·∫•p (1.827)")
print("   - √ù nghƒ©a: Nh√≥m c√≥ √Ω ƒë·ªãnh mua cao, tin t∆∞·ªüng v√† th√≠ch th√∫ v·ªõi mua s·∫Øm online")

# Cluster 1: T·∫•t c·∫£ ch·ªâ s·ªë th·∫•p nh·∫•t
print("\nüòü Cluster 1: 'Skeptical Browsers' (Ng∆∞·ªùi duy·ªát web nghi ng·ªù)")
print("   - ƒê·∫∑c ƒëi·ªÉm: INT th·∫•p nh·∫•t (3.114), TRUST th·∫•p nh·∫•t (2.871), CONV th·∫•p nh·∫•t (3.393)")
print("   - VE cao nh·∫•t (2.569), NVSE cao nh·∫•t (2.979)")
print("   - √ù nghƒ©a: Nh√≥m √≠t tin t∆∞·ªüng, √≠t √Ω ƒë·ªãnh mua, lo l·∫Øng v·ªÅ r·ªßi ro v√† t·ª± ƒë√°nh gi√° ti√™u c·ª±c")

# Cluster 2: INT cao nh·∫•t, VE th·∫•p nh·∫•t, c√°c ch·ªâ s·ªë d∆∞∆°ng t√≠nh cao nh·∫•t
print("\n‚≠ê Cluster 2: 'Convenience Seekers' (Ng∆∞·ªùi t√¨m ki·∫øm s·ª± ti·ªán l·ª£i)")
print("   - ƒê·∫∑c ƒëi·ªÉm: INT cao nh·∫•t (4.641), CONV cao nh·∫•t (4.766), ENJ cao nh·∫•t (4.970)")
print("   - TRUST cao nh·∫•t (4.308), VE th·∫•p nh·∫•t (1.624), NVSE th·∫•p nh·∫•t (1.468)")
print("   - √ù nghƒ©a: Nh√≥m ho√†n h·∫£o - tin t∆∞·ªüng cao, r·ªßi ro th·∫•p, y√™u th√≠ch s·ª± ti·ªán l·ª£i")

# T·∫°o dictionary mapping cluster number sang t√™n
cluster_names = {
    0: 'Enthusiastic Shoppers',
    1: 'Skeptical Browsers',
    2: 'Convenience Seekers'
}

# Th√™m c·ªôt t√™n cluster v√†o dataframe
df['cluster_name'] = df['cluster'].map(cluster_names)

# Hi·ªÉn th·ªã ph√¢n b·ªë
print("\n" + "="*70)
print("PH√ÇN B·ªê S·ªê L∆Ø·ª¢NG KH√ÅCH H√ÄNG THEO C·ª§M:")
print("="*70)
cluster_distribution = df.groupby(['cluster', 'cluster_name']).size().reset_index(name='S·ªë l∆∞·ª£ng')
cluster_distribution['T·ª∑ l·ªá (%)'] = (cluster_distribution['S·ªë l∆∞·ª£ng'] / len(df) * 100).round(2)
display(cluster_distribution)

# T·∫°o visualization cho cluster names
plt.figure(figsize=(10, 6))
counts = df['cluster_name'].value_counts()
colors = ['#2ecc71', '#e74c3c', '#3498db']  # M√†u cho m·ªói cluster
plt.bar(counts.index, counts.values, color=colors, edgecolor='black', linewidth=1.5)
plt.xlabel('T√™n Cluster', fontsize=12, fontweight='bold')
plt.ylabel('S·ªë l∆∞·ª£ng kh√°ch h√†ng', fontsize=12, fontweight='bold')
plt.title('Ph√¢n b·ªë kh√°ch h√†ng theo c√°c nh√≥m', fontsize=14, fontweight='bold')
plt.xticks(rotation=15, ha='right')

# Th√™m s·ªë l∆∞·ª£ng tr√™n m·ªói c·ªôt
for i, (name, value) in enumerate(counts.items()):
    plt.text(i, value + 2, str(value), ha='center', fontweight='bold', fontsize=11)

plt.tight_layout()
plt.show()

print("\n‚úÖ ƒê√£ ho√†n th√†nh vi·ªác ƒë·∫∑t t√™n c√°c cluster!")
print("C·ªôt 'cluster_name' ƒë√£ ƒë∆∞·ª£c th√™m v√†o dataframe.")

# ============================================================
# 5.2. MARKETING INSIGHTS & RECOMMENDATIONS
# ============================================================

print("="*80)
print("üéØ CHI·∫æN L∆Ø·ª¢C MARKETING CHO T·ª™NG NH√ìM KH√ÅCH H√ÄNG")
print("="*80)

print("\n" + "-"*80)
print("üìä CLUSTER 0: 'ENTHUSIASTIC SHOPPERS' (Ng∆∞·ªùi mua s·∫Øm nhi·ªát t√¨nh)")
print(f"   S·ªë l∆∞·ª£ng: {len(df[df['cluster']==0])} kh√°ch h√†ng ({len(df[df['cluster']==0])/len(df)*100:.1f}%)")
print("-"*80)
print("üí° Chi·∫øn l∆∞·ª£c:")
print("   ‚Ä¢ Loyalty Programs: T·∫°o ch∆∞∆°ng tr√¨nh kh√°ch h√†ng th√¢n thi·∫øt v·ªõi ∆∞u ƒë√£i ƒë·∫∑c bi·ªát")
print("   ‚Ä¢ Premium Services: Cung c·∫•p d·ªãch v·ª• giao h√†ng nhanh, free shipping")
print("   ‚Ä¢ Exclusive Offers: G·ª≠i ∆∞u ƒë√£i ƒë·ªôc quy·ªÅn, flash sale s·ªõm nh·∫•t")
print("   ‚Ä¢ Social Proof: Thu th·∫≠p review t√≠ch c·ª±c t·ª´ nh√≥m n√†y")
print("   ‚Ä¢ Upselling/Cross-selling: Gi·ªõi thi·ªáu s·∫£n ph·∫©m cao c·∫•p h∆°n")

print("\n" + "-"*80)
print("üìä CLUSTER 1: 'SKEPTICAL BROWSERS' (Ng∆∞·ªùi duy·ªát web nghi ng·ªù)")
print(f"   S·ªë l∆∞·ª£ng: {len(df[df['cluster']==1])} kh√°ch h√†ng ({len(df[df['cluster']==1])/len(df)*100:.1f}%)")
print("-"*80)
print("üí° Chi·∫øn l∆∞·ª£c:")
print("   ‚Ä¢ Trust Building: Hi·ªÉn th·ªã ch·ª©ng ch·ªâ b·∫£o m·∫≠t, ƒë·∫£m b·∫£o ho√†n ti·ªÅn")
print("   ‚Ä¢ Risk Reduction: Ch√≠nh s√°ch ƒë·ªïi tr·∫£ linh ho·∫°t, d√πng th·ª≠ mi·ªÖn ph√≠")
print("   ‚Ä¢ Social Proof: Hi·ªÉn th·ªã review, rating, s·ªë ng∆∞·ªùi ƒë√£ mua")
print("   ‚Ä¢ First Purchase Incentives: Gi·∫£m gi√° l·∫ßn ƒë·∫ßu, freeship ƒë∆°n ƒë·∫ßu")
print("   ‚Ä¢ Education: H∆∞·ªõng d·∫´n chi ti·∫øt v·ªÅ s·∫£n ph·∫©m, FAQ ƒë·∫ßy ƒë·ªß")
print("   ‚Ä¢ Customer Support: H·ªó tr·ª£ 24/7, chatbot th√¥ng minh")

print("\n" + "-"*80)
print("üìä CLUSTER 2: 'CONVENIENCE SEEKERS' (Ng∆∞·ªùi t√¨m ki·∫øm s·ª± ti·ªán l·ª£i)")
print(f"   S·ªë l∆∞·ª£ng: {len(df[df['cluster']==2])} kh√°ch h√†ng ({len(df[df['cluster']==2])/len(df)*100:.1f}%)")
print("-"*80)
print("üí° Chi·∫øn l∆∞·ª£c:")
print("   ‚Ä¢ Convenience Features: 1-click checkout, l∆∞u th√¥ng tin thanh to√°n")
print("   ‚Ä¢ Fast Delivery: Giao h√†ng trong ng√†y, express delivery")
print("   ‚Ä¢ Mobile Optimization: App mobile m∆∞·ª£t m√†, th√¢n thi·ªán")
print("   ‚Ä¢ Subscription Model: ƒêƒÉng k√Ω nh·∫≠n h√†ng ƒë·ªãnh k·ª≥ (auto-replenish)")
print("   ‚Ä¢ Personalization: G·ª£i √Ω s·∫£n ph·∫©m d·ª±a tr√™n l·ªãch s·ª≠ mua")
print("   ‚Ä¢ Premium Experience: VIP support, dedicated account manager")

print("\n" + "="*80)
print("‚úÖ K·∫æT LU·∫¨N T·ªîNG QUAN")
print("="*80)
print("""
‚Ä¢ Cluster 2 (Convenience Seekers): Nh√≥m VIP - ƒê·∫ßu t∆∞ m·∫°nh nh·∫•t
‚Ä¢ Cluster 0 (Enthusiastic Shoppers): Nh√≥m ti·ªÅm nƒÉng - Duy tr√¨ & ph√°t tri·ªÉn
‚Ä¢ Cluster 1 (Skeptical Browsers): Nh√≥m c·∫ßn chuy·ªÉn ƒë·ªïi - T·∫≠p trung x√¢y d·ª±ng l√≤ng tin

üéØ ∆Øu ti√™n:
1. Gi·ªØ ch√¢n v√† ph√°t tri·ªÉn Cluster 2 (Revenue cao nh·∫•t)
2. N√¢ng c·∫•p Cluster 0 l√™n Cluster 2
3. Chuy·ªÉn ƒë·ªïi Cluster 1 th√†nh kh√°ch h√†ng trung th√†nh
""")
print("="*80)

# ============================================================
# KMEANS ‚Äî V·∫º C√ÅC C·ª§M B·∫∞NG PCA 2D
# ============================================================
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

pca = PCA(n_components=2)
pca_result = pca.fit_transform(X_scaled)

df['pca1'] = pca_result[:, 0]
df['pca2'] = pca_result[:, 1]

plt.figure(figsize=(8,6))
for c in range(k):
    plt.scatter(
        df[df['cluster'] == c]['pca1'],
        df[df['cluster'] == c]['pca2'],
        label=f'C·ª•m {c}',
        alpha=0.7
    )
plt.title("Bi·ªÉu ƒë·ªì PCA ‚Äî Ph√¢n c·ª•m K-Means")
plt.xlabel("PCA 1")
plt.ylabel("PCA 2")
plt.legend()
plt.show()

# ============================================================
# SCATTER 2 BI·∫æN (TRUST ‚Äì ENJOYMENT)
# ============================================================

plt.figure(figsize=(8,6))
for c in range(k):
    plt.scatter(
        df[df['cluster']==c]['TRUST'],
        df[df['cluster']==c]['ENJ'],
        label=f'C·ª•m {c}',
        alpha=0.7
    )
plt.title("Scatter Plot: TRUST vs ENJ theo t·ª´ng c·ª•m")
plt.xlabel("TRUST")
plt.ylabel("ENJOYMENT")
plt.legend()
plt.show()

# ============================================================
# V·∫º RADAR CHART CHO TRUNG B√åNH C√ÅC THANG ƒêO
# ============================================================
import numpy as np

cluster_mean = df.groupby('cluster')[cluster_vars].mean()

labels = cluster_vars
num_vars = len(labels)

angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()
angles += angles[:1]

plt.figure(figsize=(10,8))

for i, row in cluster_mean.iterrows():
    values = row.tolist()
    values += values[:1]
    plt.polar(angles, values, marker='o', label=f'C·ª•m {i}')

plt.title("Radar Chart ‚Äî ƒê·∫∑c ƒëi·ªÉm trung b√¨nh c√°c c·ª•m")
plt.legend(loc='upper right', bbox_to_anchor=(1.2, 1.1))
plt.show()

# ============================================================
# SILHOUETTE SCORE ‚Äî ƒê√ÅNH GI√Å CH·∫§T L∆Ø·ª¢NG PH√ÇN C·ª§M
# ============================================================
from sklearn.metrics import silhouette_score

score = silhouette_score(X_scaled, df['cluster'])
print("Silhouette Score =", score)

# ============================================================
# SILHOUETTE PLOT
# ============================================================
from sklearn.metrics import silhouette_samples
import matplotlib.cm as cm

silhouette_vals = silhouette_samples(X_scaled, df['cluster'])

plt.figure(figsize=(8,6))
y_lower = 10

for i in range(k):
    ith_cluster_silhouette = silhouette_vals[df['cluster'] == i]
    ith_cluster_silhouette.sort()

    size_cluster_i = ith_cluster_silhouette.shape[0]
    y_upper = y_lower + size_cluster_i

    color = cm.nipy_spectral(float(i) / k)
    plt.fill_betweenx(
        np.arange(y_lower, y_upper),
        0,
        ith_cluster_silhouette,
        facecolor=color,
        alpha=0.7
    )
    plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
    y_lower = y_upper + 10

plt.axvline(score, color="red", linestyle="--")
plt.title("Silhouette Plot cho K-Means")
plt.xlabel("Silhouette Coefficient")
plt.ylabel("Cluster")
plt.show()

# ============================================================
# 6. MULTIPLE REGRESSION predicting AL
# ============================================================
import statsmodels.api as sm

df['AL'] = df[['al1','al2','al3']].mean(axis=1)

# Platform Features ‚Üí AL
X1 = df[['INT','INF','VE','NVSE']]
y1 = df['AL']
X1 = sm.add_constant(X1)

model1 = sm.OLS(y1, X1).fit()
print(model1.summary())

# ============================================================
# 7. LOGISTIC REGRESSION predicting target
# ============================================================
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

X2 = df[['TRUST','CONV','ENJ','SC']]
y2 = df['target']

logit = LogisticRegression(max_iter=1000)
logit.fit(X2, y2)

y_pred = logit.predict(X2)
print(classification_report(y2, y_pred))

from sklearn.neural_network import MLPClassifier

ann = MLPClassifier(hidden_layer_sizes=(8,4), activation='relu', solver='adam',
                    max_iter=1000, random_state=42)

ann.fit(X2, y2)
y_pred_ann = ann.predict(X2)

print("\nK·∫øt qu·∫£ ANN:")
print(classification_report(y2, y_pred_ann))

